# -*- coding: utf-8 -*-
"""Copy of PonderNet - CIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mOZiOWXEfTFab_MU5YB3wp2-3fl6NXFP

This code is adopted from one of the [PyTorch Lightning examples](https://colab.research.google.com/drive/1Tr9dYlwBKk6-LgLKGO8KYZULnguVA992?usp=sharing#scrollTo=CxXtBfFrKYgA) and [this PonderNet implementation](https://nn.labml.ai/adaptive_computation/ponder_net/index.html).

# Setup and imports

We use `PyTorch Lightning` (wrapping `PyTorch`) as our main framework and `wandb` to track and log the experiments. We set all seeds through `PyTorch Lightning`'s dedicated function.
"""

# import Libraries
import os

# torch imports
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from pl_bolts.datamodules import CIFAR10DataModule
from pl_bolts.transforms.dataset_normalizations import cifar10_normalization
from pytorch_lightning import LightningModule, Trainer, seed_everything
from pytorch_lightning.callbacks import LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
from torch.optim.lr_scheduler import OneCycleLR
from torch.optim.swa_utils import AveragedModel, update_bn
from torchmetrics.functional import accuracy
import torch.nn.functional as F
import torchmetrics
import torchvision

# pl imports
import pytorch_lightning as pl
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint

# pondernet model
from pondernet import *

# remaining imports
import wandb
from math import floor

# set seeds
seed_everything(7)

# log in to wandb
wandb.login()

"""# Constants and hyeperparameters

We define the hyperparameters for our experiments. The choices for the underlying CNN are taken from the linked MNIST tutorial, and similarly with the PonderNet hyperparameters. 
"""

PATH_DATASETS = os.environ.get("PATH_DATASETS", ".")

# TRAINER SETTINGS
AVAIL_GPUS = torch.cuda.device_count()
BATCH_SIZE = 128 if AVAIL_GPUS else 64
NUM_WORKERS = int(os.cpu_count() / 2)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

EPOCHS = 5    # change 20 - 40

# OPTIMIZER SETTINGS
LR = 0.001
GRAD_NORM_CLIP = 0.5

# MODEL HPARAMS
N_ELEMS = 512
N_HIDDEN = 100

KERNEL_SIZE = 5

MAX_STEPS = 20    # decrease this
LAMBDA_P = 0.2    # change to get 91%
BETA = 0.01

"""# CIFAR10

"""

train_transforms = torchvision.transforms.Compose(
    [
        torchvision.transforms.RandomCrop(32, padding=4),
        torchvision.transforms.RandomHorizontalFlip(),
        torchvision.transforms.ToTensor(),
        cifar10_normalization(),
    ]
)

test_transforms = torchvision.transforms.Compose(
    [
        torchvision.transforms.ToTensor(),
        cifar10_normalization(),
    ]
)

"""# Run interpolation

Load the MNIST dataset with no rotations and train PonderNet on it. Make sure to edit the `WandbLogger` call so that you log the experiment on your account's desired project.
"""

cifar10_dm = CIFAR10DataModule(
    data_dir=PATH_DATASETS,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS,
    train_transforms=train_transforms,
    test_transforms=test_transforms,
    val_transforms=test_transforms,
)

# initialize datamodule and model
model = PonderCIFAR(n_elems     = N_ELEMS,
                    n_hidden    = N_HIDDEN,
                    max_steps   = MAX_STEPS,
                    lambda_p    = LAMBDA_P,
                    beta        = BETA,
                    lr          = LR)

# setup logger
logger = WandbLogger(project='PonderNet', name='interpolation', offline=False)
logger.watch(model)

trainer = Trainer(
    gpus=-1,                            # use all available GPU's
    max_epochs=EPOCHS,                  # maximum number of epochs
    gradient_clip_val=GRAD_NORM_CLIP,   # gradient clipping
    val_check_interval=0.25,            # validate 4 times per epoch
    precision=16,                       # train in half precision
    deterministic=True)                 # for reproducibility

# fit the model
trainer.fit(model, datamodule=cifar10_dm)

# evaluate on the test set
trainer.test(model, datamodule=cifar10_dm)

wandb.finish()